{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b4e1e",
   "metadata": {},
   "source": [
    "# Chapter 7\n",
    "\n",
    "This notebook contains all Python files from ch07 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8456",
   "metadata": {},
   "source": [
    "## show_addition_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8c123",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom dataset import sequence\n\n\n(x_train, t_train), (x_test, t_test) = \\\n    sequence.load_data('addition.txt', seed=1984)\nchar_to_id, id_to_char = sequence.get_vocab()\n\nprint(x_train.shape, t_train.shape)\nprint(x_test.shape, t_test.shape)\n# (45000, 7) (45000, 5)\n# (5000, 7) (5000, 5)\n\nprint(x_train[0])\nprint(t_train[0])\n# [ 3  0  2  0  0 11  5]\n# [ 6  0 11  7  5]\n\nprint(''.join([id_to_char[c] for c in x_train[0]]))\nprint(''.join([id_to_char[c] for c in t_train[0]]))\n# 71+118\n# _189"
  },
  {
   "cell_type": "markdown",
   "id": "a1b3d452",
   "metadata": {},
   "source": [
    "## rnnlm_gen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4e567",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom common.functions import softmax\n# Note: This cell requires Rnnlm and BetterRnnlm classes to be defined first (see ch06 notebook)\n\n\nclass RnnlmGen(Rnnlm):\n    def generate(self, start_id, skip_ids=None, sample_size=100):\n        word_ids = [start_id]\n\n        x = start_id\n        while len(word_ids) < sample_size:\n            x = np.array(x).reshape(1, 1)\n            score = self.predict(x)\n            p = softmax(score.flatten())\n\n            sampled = np.random.choice(len(p), size=1, p=p)\n            if (skip_ids is None) or (sampled not in skip_ids):\n                x = sampled\n                word_ids.append(int(x))\n\n        return word_ids\n\n    def get_state(self):\n        return self.lstm_layer.h, self.lstm_layer.c\n\n    def set_state(self, state):\n        self.lstm_layer.set_state(*state)\n\n\nclass BetterRnnlmGen(BetterRnnlm):\n    def generate(self, start_id, skip_ids=None, sample_size=100):\n        word_ids = [start_id]\n\n        x = start_id\n        while len(word_ids) < sample_size:\n            x = np.array(x).reshape(1, 1)\n            score = self.predict(x).flatten()\n            p = softmax(score).flatten()\n\n            sampled = np.random.choice(len(p), size=1, p=p)\n            if (skip_ids is None) or (sampled not in skip_ids):\n                x = sampled\n                word_ids.append(int(x))\n\n        return word_ids\n\n    def get_state(self):\n        states = []\n        for layer in self.lstm_layers:\n            states.append((layer.h, layer.c))\n        return states\n\n    def set_state(self, states):\n        for layer, state in zip(self.lstm_layers, states):\n            layer.set_state(*state)"
  },
  {
   "cell_type": "markdown",
   "id": "c3d4f789",
   "metadata": {},
   "source": [
    "## generate_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f891",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom dataset import ptb\n# Note: RnnlmGen is defined in the rnnlm_gen.py cell above\n\n\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\nvocab_size = len(word_to_id)\ncorpus_size = len(corpus)\n\nmodel = RnnlmGen()\nmodel.load_params('../ch06/Rnnlm.pkl')\n\n# start文字とskip文字の設定\nstart_word = 'you'\nstart_id = word_to_id[start_word]\nskip_words = ['N', '<unk>', '$']\nskip_ids = [word_to_id[w] for w in skip_words]\n# 文章生成\nword_ids = model.generate(start_id, skip_ids)\ntxt = ' '.join([id_to_word[i] for i in word_ids])\ntxt = txt.replace(' <eos>', '.\\n')\nprint(txt)"
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a123",
   "metadata": {},
   "source": [
    "## train_seq2seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2c456",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset import sequence\nfrom common.optimizer import Adam\nfrom common.trainer import Trainer\nfrom common.util import eval_seq2seq\n# Note: Seq2seq and PeekySeq2seq classes are defined in the cells below\n\n\n# データセットの読み込み\n(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\nchar_to_id, id_to_char = sequence.get_vocab()\n\n# Reverse input? =================================================\nis_reverse = False  # True\nif is_reverse:\n    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n# ================================================================\n\n# ハイパーパラメータの設定\nvocab_size = len(char_to_id)\nwordvec_size = 16\nhidden_size = 128\nbatch_size = 128\nmax_epoch = 25\nmax_grad = 5.0\n\n# Normal or Peeky? ==============================================\nmodel = Seq2seq(vocab_size, wordvec_size, hidden_size)\n# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n# ================================================================\noptimizer = Adam()\ntrainer = Trainer(model, optimizer)\n\nacc_list = []\nfor epoch in range(max_epoch):\n    trainer.fit(x_train, t_train, max_epoch=1,\n                batch_size=batch_size, max_grad=max_grad)\n\n    correct_num = 0\n    for i in range(len(x_test)):\n        question, correct = x_test[[i]], t_test[[i]]\n        verbose = i < 10\n        correct_num += eval_seq2seq(model, question, correct,\n                                    id_to_char, verbose, is_reverse)\n\n    acc = float(correct_num) / len(x_test)\n    acc_list.append(acc)\n    print('val acc %.3f%%' % (acc * 100))\n\n# グラフの描画\nx = np.arange(len(acc_list))\nplt.plot(x, acc_list, marker='o')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.ylim(0, 1.0)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "a4b7d123",
   "metadata": {},
   "source": [
    "## peeky_seq2seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5e234",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.time_layers import *\n# Note: Seq2seq and Encoder classes are defined in the seq2seq.py cell below\n\n\nclass PeekyDecoder:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b = np.zeros(4 * H).astype('f')\n        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        self.embed = TimeEmbedding(embed_W)\n        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n        self.affine = TimeAffine(affine_W, affine_b)\n\n        self.params, self.grads = [], []\n        for layer in (self.embed, self.lstm, self.affine):\n            self.params += layer.params\n            self.grads += layer.grads\n        self.cache = None\n\n    def forward(self, xs, h):\n        N, T = xs.shape\n        N, H = h.shape\n\n        self.lstm.set_state(h)\n\n        out = self.embed.forward(xs)\n        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n        out = np.concatenate((hs, out), axis=2)\n\n        out = self.lstm.forward(out)\n        out = np.concatenate((hs, out), axis=2)\n\n        score = self.affine.forward(out)\n        self.cache = H\n        return score\n\n    def backward(self, dscore):\n        H = self.cache\n\n        dout = self.affine.backward(dscore)\n        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n        dout = self.lstm.backward(dout)\n        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n        self.embed.backward(dembed)\n\n        dhs = dhs0 + dhs1\n        dh = self.lstm.dh + np.sum(dhs, axis=1)\n        return dh\n\n    def generate(self, h, start_id, sample_size):\n        sampled = []\n        char_id = start_id\n        self.lstm.set_state(h)\n\n        H = h.shape[1]\n        peeky_h = h.reshape(1, 1, H)\n        for _ in range(sample_size):\n            x = np.array([char_id]).reshape((1, 1))\n            out = self.embed.forward(x)\n\n            out = np.concatenate((peeky_h, out), axis=2)\n            out = self.lstm.forward(out)\n            out = np.concatenate((peeky_h, out), axis=2)\n            score = self.affine.forward(out)\n\n            char_id = np.argmax(score.flatten())\n            sampled.append(char_id)\n\n        return sampled\n\n\nclass PeekySeq2seq(Seq2seq):\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        self.encoder = Encoder(V, D, H)\n        self.decoder = PeekyDecoder(V, D, H)\n        self.softmax = TimeSoftmaxWithLoss()\n\n        self.params = self.encoder.params + self.decoder.params\n        self.grads = self.encoder.grads + self.decoder.grads"
  },
  {
   "cell_type": "markdown",
   "id": "c1a2b345",
   "metadata": {},
   "source": [
    "## generate_better_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7e890",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.np import *\nfrom dataset import ptb\n# Note: BetterRnnlmGen is defined in the rnnlm_gen.py cell above\n\n\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\nvocab_size = len(word_to_id)\ncorpus_size = len(corpus)\n\n\nmodel = BetterRnnlmGen()\nmodel.load_params('../ch06/BetterRnnlm.pkl')\n\n# start文字とskip文字の設定\nstart_word = 'you'\nstart_id = word_to_id[start_word]\nskip_words = ['N', '<unk>', '$']\nskip_ids = [word_to_id[w] for w in skip_words]\n# 文章生成\nword_ids = model.generate(start_id, skip_ids)\ntxt = ' '.join([id_to_word[i] for i in word_ids])\ntxt = txt.replace(' <eos>', '.\\n')\n\nprint(txt)\n\n\nmodel.reset_state()\n\nstart_words = 'the meaning of life is'\nstart_ids = [word_to_id[w] for w in start_words.split(' ')]\n\nfor x in start_ids[:-1]:\n    x = np.array(x).reshape(1, 1)\n    model.predict(x)\n\nword_ids = model.generate(start_ids[-1], skip_ids)\nword_ids = start_ids[:-1] + word_ids\ntxt = ' '.join([id_to_word[i] for i in word_ids])\ntxt = txt.replace(' <eos>', '.\\n')\nprint('-' * 50)\nprint(txt)"
  },
  {
   "cell_type": "markdown",
   "id": "e8b9c456",
   "metadata": {},
   "source": [
    "## seq2seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b234",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.time_layers import *\nfrom common.base_model import BaseModel\n\n\nclass Encoder:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b = np.zeros(4 * H).astype('f')\n\n        self.embed = TimeEmbedding(embed_W)\n        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n\n        self.params = self.embed.params + self.lstm.params\n        self.grads = self.embed.grads + self.lstm.grads\n        self.hs = None\n\n    def forward(self, xs):\n        xs = self.embed.forward(xs)\n        hs = self.lstm.forward(xs)\n        self.hs = hs\n        return hs[:, -1, :]\n\n    def backward(self, dh):\n        dhs = np.zeros_like(self.hs)\n        dhs[:, -1, :] = dh\n\n        dout = self.lstm.backward(dhs)\n        dout = self.embed.backward(dout)\n        return dout\n\n\nclass Decoder:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b = np.zeros(4 * H).astype('f')\n        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        self.embed = TimeEmbedding(embed_W)\n        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n        self.affine = TimeAffine(affine_W, affine_b)\n\n        self.params, self.grads = [], []\n        for layer in (self.embed, self.lstm, self.affine):\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, h):\n        self.lstm.set_state(h)\n\n        out = self.embed.forward(xs)\n        out = self.lstm.forward(out)\n        score = self.affine.forward(out)\n        return score\n\n    def backward(self, dscore):\n        dout = self.affine.backward(dscore)\n        dout = self.lstm.backward(dout)\n        dout = self.embed.backward(dout)\n        dh = self.lstm.dh\n        return dh\n\n    def generate(self, h, start_id, sample_size):\n        sampled = []\n        sample_id = start_id\n        self.lstm.set_state(h)\n\n        for _ in range(sample_size):\n            x = np.array(sample_id).reshape((1, 1))\n            out = self.embed.forward(x)\n            out = self.lstm.forward(out)\n            score = self.affine.forward(out)\n\n            sample_id = np.argmax(score.flatten())\n            sampled.append(int(sample_id))\n\n        return sampled\n\n\nclass Seq2seq(BaseModel):\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        self.encoder = Encoder(V, D, H)\n        self.decoder = Decoder(V, D, H)\n        self.softmax = TimeSoftmaxWithLoss()\n\n        self.params = self.encoder.params + self.decoder.params\n        self.grads = self.encoder.grads + self.decoder.grads\n\n    def forward(self, xs, ts):\n        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n\n        h = self.encoder.forward(xs)\n        score = self.decoder.forward(decoder_xs, h)\n        loss = self.softmax.forward(score, decoder_ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.softmax.backward(dout)\n        dh = self.decoder.backward(dout)\n        dout = self.encoder.backward(dh)\n        return dout\n\n    def generate(self, xs, start_id, sample_size):\n        h = self.encoder.forward(xs)\n        sampled = self.decoder.generate(h, start_id, sample_size)\n        return sampled"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}