{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b4e1e",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "This notebook contains all Python files from ch04 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8456",
   "metadata": {},
   "source": [
    "## negative_sampling_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8c123",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.np import *  # import numpy as np\nfrom common.layers import Embedding, SigmoidWithLoss\nimport collections\n\n\nclass EmbeddingDot:\n    def __init__(self, W):\n        self.embed = Embedding(W)\n        self.params = self.embed.params\n        self.grads = self.embed.grads\n        self.cache = None\n\n    def forward(self, h, idx):\n        target_W = self.embed.forward(idx)\n        out = np.sum(target_W * h, axis=1)\n\n        self.cache = (h, target_W)\n        return out\n\n    def backward(self, dout):\n        h, target_W = self.cache\n        dout = dout.reshape(dout.shape[0], 1)\n\n        dtarget_W = dout * h\n        self.embed.backward(dtarget_W)\n        dh = dout * target_W\n        return dh\n\n\nclass UnigramSampler:\n    def __init__(self, corpus, power, sample_size):\n        self.sample_size = sample_size\n        self.vocab_size = None\n        self.word_p = None\n\n        counts = collections.Counter()\n        for word_id in corpus:\n            counts[word_id] += 1\n\n        vocab_size = len(counts)\n        self.vocab_size = vocab_size\n\n        self.word_p = np.zeros(vocab_size)\n        for i in range(vocab_size):\n            self.word_p[i] = counts[i]\n\n        self.word_p = np.power(self.word_p, power)\n        self.word_p /= np.sum(self.word_p)\n\n    def get_negative_sample(self, target):\n        batch_size = target.shape[0]\n\n        if not GPU:\n            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n\n            for i in range(batch_size):\n                p = self.word_p.copy()\n                target_idx = target[i]\n                p[target_idx] = 0\n                p /= p.sum()\n                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n        else:\n            # GPU(cupy）で計算するときは、速度を優先\n            # 負例にターゲットが含まれるケースがある\n            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n                                               replace=True, p=self.word_p)\n\n        return negative_sample\n\n\nclass NegativeSamplingLoss:\n    def __init__(self, W, corpus, power=0.75, sample_size=5):\n        self.sample_size = sample_size\n        self.sampler = UnigramSampler(corpus, power, sample_size)\n        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n\n        self.params, self.grads = [], []\n        for layer in self.embed_dot_layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, h, target):\n        batch_size = target.shape[0]\n        negative_sample = self.sampler.get_negative_sample(target)\n\n        # 正例のフォワード\n        score = self.embed_dot_layers[0].forward(h, target)\n        correct_label = np.ones(batch_size, dtype=np.int32)\n        loss = self.loss_layers[0].forward(score, correct_label)\n\n        # 負例のフォワード\n        negative_label = np.zeros(batch_size, dtype=np.int32)\n        for i in range(self.sample_size):\n            negative_target = negative_sample[:, i]\n            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n            loss += self.loss_layers[1 + i].forward(score, negative_label)\n\n        return loss\n\n    def backward(self, dout=1):\n        dh = 0\n        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n            dscore = l0.backward(dout)\n            dh += l1.backward(dscore)\n\n        return dh"
  },
  {
   "cell_type": "markdown",
   "id": "a1b3d452",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4e567",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common import config\n# GPUで実行する場合は、下記のコメントアウトを消去（要cupy）\n# ===============================================\n# config.GPU = True\n# ===============================================\nfrom common.np import *\nimport pickle\nfrom common.trainer import Trainer\nfrom common.optimizer import Adam\nfrom common.util import create_contexts_target, to_cpu, to_gpu\nfrom dataset import ptb\n# Note: CBOW and SkipGram classes are defined in the cbow.py and skip_gram.py cells above\n\n\n# ハイパーパラメータの設定\nwindow_size = 5\nhidden_size = 100\nbatch_size = 100\nmax_epoch = 10\n\n# データの読み込み\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\nvocab_size = len(word_to_id)\n\ncontexts, target = create_contexts_target(corpus, window_size)\nif config.GPU:\n    contexts, target = to_gpu(contexts), to_gpu(target)\n\n# モデルなどの生成\nmodel = CBOW(vocab_size, hidden_size, window_size, corpus)\n# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\noptimizer = Adam()\ntrainer = Trainer(model, optimizer)\n\n# 学習開始\ntrainer.fit(contexts, target, max_epoch, batch_size)\ntrainer.plot()\n\n# 後ほど利用できるように、必要なデータを保存\nword_vecs = model.word_vecs\nif config.GPU:\n    word_vecs = to_cpu(word_vecs)\nparams = {}\nparams['word_vecs'] = word_vecs.astype(np.float16)\nparams['word_to_id'] = word_to_id\nparams['id_to_word'] = id_to_word\npkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\nwith open(pkl_file, 'wb') as f:\n    pickle.dump(params, f, -1)"
  },
  {
   "cell_type": "markdown",
   "id": "c3d4f789",
   "metadata": {},
   "source": [
    "## skip_gram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f891",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.layers import *\n# Note: NegativeSamplingLoss is defined in the negative_sampling_layer.py cell above\n\n\nclass SkipGram:\n    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n        V, H = vocab_size, hidden_size\n        rn = np.random.randn\n\n        # 重みの初期化\n        W_in = 0.01 * rn(V, H).astype('f')\n        W_out = 0.01 * rn(V, H).astype('f')\n\n        # レイヤの生成\n        self.in_layer = Embedding(W_in)\n        self.loss_layers = []\n        for i in range(2 * window_size):\n            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n            self.loss_layers.append(layer)\n\n        # すべての重みと勾配をリストにまとめる\n        layers = [self.in_layer] + self.loss_layers\n        self.params, self.grads = [], []\n        for layer in layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n        # メンバ変数に単語の分散表現を設定\n        self.word_vecs = W_in\n\n    def forward(self, contexts, target):\n        h = self.in_layer.forward(target)\n\n        loss = 0\n        for i, layer in enumerate(self.loss_layers):\n            loss += layer.forward(h, contexts[:, i])\n        return loss\n\n    def backward(self, dout=1):\n        dh = 0\n        for i, layer in enumerate(self.loss_layers):\n            dh += layer.backward(dout)\n        self.in_layer.backward(dh)\n        return None"
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a123",
   "metadata": {},
   "source": [
    "## eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2c456",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.util import most_similar, analogy\nimport pickle\n\n\npkl_file = 'cbow_params.pkl'\n# pkl_file = 'skipgram_params.pkl'\n\nwith open(pkl_file, 'rb') as f:\n    params = pickle.load(f)\n    word_vecs = params['word_vecs']\n    word_to_id = params['word_to_id']\n    id_to_word = params['id_to_word']\n\n# most similar task\nquerys = ['you', 'year', 'car', 'toyota']\nfor query in querys:\n    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n\n# analogy task\nprint('-'*50)\nanalogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\nanalogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\nanalogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\nanalogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
  },
  {
   "cell_type": "markdown",
   "id": "a4b7d123",
   "metadata": {},
   "source": [
    "## cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5e234",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.np import *  # import numpy as np\nfrom common.layers import Embedding\n# Note: NegativeSamplingLoss is defined in the negative_sampling_layer.py cell above\n\n\nclass CBOW:\n    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n        V, H = vocab_size, hidden_size\n\n        # 重みの初期化\n        W_in = 0.01 * np.random.randn(V, H).astype('f')\n        W_out = 0.01 * np.random.randn(V, H).astype('f')\n\n        # レイヤの生成\n        self.in_layers = []\n        for i in range(2 * window_size):\n            layer = Embedding(W_in)  # Embeddingレイヤを使用\n            self.in_layers.append(layer)\n        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n\n        # すべての重みと勾配をリストにまとめる\n        layers = self.in_layers + [self.ns_loss]\n        self.params, self.grads = [], []\n        for layer in layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n        # メンバ変数に単語の分散表現を設定\n        self.word_vecs = W_in\n\n    def forward(self, contexts, target):\n        h = 0\n        for i, layer in enumerate(self.in_layers):\n            h += layer.forward(contexts[:, i])\n        h *= 1 / len(self.in_layers)\n        loss = self.ns_loss.forward(h, target)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.ns_loss.backward(dout)\n        dout *= 1 / len(self.in_layers)\n        for layer in self.in_layers:\n            layer.backward(dout)\n        return None"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}