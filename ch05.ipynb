{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b4e1e",
   "metadata": {},
   "source": [
    "# Chapter 5\n",
    "\n",
    "This notebook contains all Python files from ch05 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8456",
   "metadata": {},
   "source": [
    "## simple_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8c123",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom common.time_layers import *\n\n\nclass SimpleRnnlm:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        # 重みの初期化\n        embed_W = (rn(V, D) / 100).astype('f')\n        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n        rnn_b = np.zeros(H).astype('f')\n        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        # レイヤの生成\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n            TimeAffine(affine_W, affine_b)\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.rnn_layer = self.layers[1]\n\n        # すべての重みと勾配をリストにまとめる\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, ts):\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        loss = self.loss_layer.forward(xs, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        self.rnn_layer.reset_state()"
  },
  {
   "cell_type": "markdown",
   "id": "a1b3d452",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4e567",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.optimizer import SGD\nfrom common.trainer import RnnlmTrainer\nfrom dataset import ptb\n# Note: SimpleRnnlm is defined in the simple_rnnlm.py cell above\n\n\n# ハイパーパラメータの設定\nbatch_size = 10\nwordvec_size = 100\nhidden_size = 100  # RNNの隠れ状態ベクトルの要素数\ntime_size = 5  # RNNを展開するサイズ\nlr = 0.1\nmax_epoch = 100\n\n# 学習データの読み込み\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\ncorpus_size = 1000  # テスト用にデータセットを小さくする\ncorpus = corpus[:corpus_size]\nvocab_size = int(max(corpus) + 1)\nxs = corpus[:-1]  # 入力\nts = corpus[1:]  # 出力（教師ラベル）\n\n# モデルの生成\nmodel = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\noptimizer = SGD(lr)\ntrainer = RnnlmTrainer(model, optimizer)\n\ntrainer.fit(xs, ts, max_epoch, batch_size, time_size)\ntrainer.plot()"
  },
  {
   "cell_type": "markdown",
   "id": "c3d4f789",
   "metadata": {},
   "source": [
    "## train_custom_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f891",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom common.optimizer import SGD\nfrom dataset import ptb\n# Note: SimpleRnnlm is defined in the simple_rnnlm.py cell above\n\n\n# ハイパーパラメータの設定\nbatch_size = 10\nwordvec_size = 100\nhidden_size = 100\ntime_size = 5  # Truncated BPTTの展開する時間サイズ\nlr = 0.1\nmax_epoch = 100\n\n# 学習データの読み込み（データセットを小さくする）\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\ncorpus_size = 1000\ncorpus = corpus[:corpus_size]\nvocab_size = int(max(corpus) + 1)\n\nxs = corpus[:-1]  # 入力\nts = corpus[1:]  # 出力（教師ラベル）\ndata_size = len(xs)\nprint('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))\n\n# 学習時に使用する変数\nmax_iters = data_size // (batch_size * time_size)\ntime_idx = 0\ntotal_loss = 0\nloss_count = 0\nppl_list = []\n\n# モデルの生成\nmodel = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\noptimizer = SGD(lr)\n\n# ミニバッチの各サンプルの読み込み開始位置を計算\njump = (corpus_size - 1) // batch_size\noffsets = [i * jump for i in range(batch_size)]\n\nfor epoch in range(max_epoch):\n    for iter in range(max_iters):\n        # ミニバッチの取得\n        batch_x = np.empty((batch_size, time_size), dtype='i')\n        batch_t = np.empty((batch_size, time_size), dtype='i')\n        for t in range(time_size):\n            for i, offset in enumerate(offsets):\n                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n            time_idx += 1\n\n        # 勾配を求め、パラメータを更新\n        loss = model.forward(batch_x, batch_t)\n        model.backward()\n        optimizer.update(model.params, model.grads)\n        total_loss += loss\n        loss_count += 1\n\n    # エポックごとにパープレキシティの評価\n    ppl = np.exp(total_loss / loss_count)\n    print('| epoch %d | perplexity %.2f'\n          % (epoch+1, ppl))\n    ppl_list.append(float(ppl))\n    total_loss, loss_count = 0, 0\n\n# グラフの描画\nx = np.arange(len(ppl_list))\nplt.plot(x, ppl_list, label='train')\nplt.xlabel('epochs')\nplt.ylabel('perplexity')\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}