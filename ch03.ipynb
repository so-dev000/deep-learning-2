{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "\n",
    "This notebook contains all Python files from ch03 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom common.layers import MatMul, SoftmaxWithLoss\n\n\nclass SimpleCBOW:\n    def __init__(self, vocab_size, hidden_size):\n        V, H = vocab_size, hidden_size\n\n        # 重みの初期化\n        W_in = 0.01 * np.random.randn(V, H).astype('f')\n        W_out = 0.01 * np.random.randn(H, V).astype('f')\n\n        # レイヤの生成\n        self.in_layer0 = MatMul(W_in)\n        self.in_layer1 = MatMul(W_in)\n        self.out_layer = MatMul(W_out)\n        self.loss_layer = SoftmaxWithLoss()\n\n        # すべての重みと勾配をリストにまとめる\n        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n        self.params, self.grads = [], []\n        for layer in layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n        # メンバ変数に単語の分散表現を設定\n        self.word_vecs = W_in\n\n    def forward(self, contexts, target):\n        h0 = self.in_layer0.forward(contexts[:, 0])\n        h1 = self.in_layer1.forward(contexts[:, 1])\n        h = (h0 + h1) * 0.5\n        score = self.out_layer.forward(h)\n        loss = self.loss_layer.forward(score, target)\n        return loss\n\n    def backward(self, dout=1):\n        ds = self.loss_layer.backward(dout)\n        da = self.out_layer.backward(ds)\n        da *= 0.5\n        self.in_layer1.backward(da)\n        self.in_layer0.backward(da)\n        return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cbow_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom common.layers import MatMul\n\n\n# サンプルのコンテキストデータ\nc0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\nc1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n\n# 重みの初期化\nW_in = np.random.randn(7, 3)\nW_out = np.random.randn(3, 7)\n\n# レイヤの生成\nin_layer0 = MatMul(W_in)\nin_layer1 = MatMul(W_in)\nout_layer = MatMul(W_out)\n\n# 順伝搬\nh0 = in_layer0.forward(c0)\nh1 = in_layer1.forward(c1)\nh = 0.5 * (h0 + h1)\ns = out_layer.forward(h)\nprint(s)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_skip_gram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom common.layers import MatMul, SoftmaxWithLoss\n\n\nclass SimpleSkipGram:\n    def __init__(self, vocab_size, hidden_size):\n        V, H = vocab_size, hidden_size\n\n        # 重みの初期化\n        W_in = 0.01 * np.random.randn(V, H).astype('f')\n        W_out = 0.01 * np.random.randn(H, V).astype('f')\n\n        # レイヤの生成\n        self.in_layer = MatMul(W_in)\n        self.out_layer = MatMul(W_out)\n        self.loss_layer1 = SoftmaxWithLoss()\n        self.loss_layer2 = SoftmaxWithLoss()\n\n        # すべての重みと勾配をリストにまとめる\n        layers = [self.in_layer, self.out_layer]\n        self.params, self.grads = [], []\n        for layer in layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n        # メンバ変数に単語の分散表現を設定\n        self.word_vecs = W_in\n\n    def forward(self, contexts, target):\n        h = self.in_layer.forward(target)\n        s = self.out_layer.forward(h)\n        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n        loss = l1 + l2\n        return loss\n\n    def backward(self, dout=1):\n        dl1 = self.loss_layer1.backward(dout)\n        dl2 = self.loss_layer2.backward(dout)\n        ds = dl1 + dl2\n        dh = self.out_layer.backward(ds)\n        self.in_layer.backward(dh)\n        return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.trainer import Trainer\nfrom common.optimizer import Adam\nfrom common.util import preprocess, create_contexts_target, convert_one_hot\n# Note: SimpleCBOW is defined in the simple_cbow.py cell above\n\n\nwindow_size = 1\nhidden_size = 5\nbatch_size = 3\nmax_epoch = 1000\n\ntext = 'You say goodbye and I say hello.'\ncorpus, word_to_id, id_to_word = preprocess(text)\n\nvocab_size = len(word_to_id)\ncontexts, target = create_contexts_target(corpus, window_size)\ntarget = convert_one_hot(target, vocab_size)\ncontexts = convert_one_hot(contexts, vocab_size)\n\nmodel = SimpleCBOW(vocab_size, hidden_size)\noptimizer = Adam()\ntrainer = Trainer(model, optimizer)\n\ntrainer.fit(contexts, target, max_epoch, batch_size)\ntrainer.plot()\n\nword_vecs = model.word_vecs\nfor word_id, word in id_to_word.items():\n    print(word, word_vecs[word_id])"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}