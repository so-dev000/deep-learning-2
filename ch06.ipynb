{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b4e1e",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "This notebook contains all Python files from ch06 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8456",
   "metadata": {},
   "source": [
    "## train_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8c123",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.optimizer import SGD\nfrom common.trainer import RnnlmTrainer\nfrom common.util import eval_perplexity\nfrom dataset import ptb\n# Note: Rnnlm is defined in the rnnlm.py cell below\n\n\n# ハイパーパラメータの設定\nbatch_size = 20\nwordvec_size = 100\nhidden_size = 100  # RNNの隠れ状態ベクトルの要素数\ntime_size = 35  # RNNを展開するサイズ\nlr = 20.0\nmax_epoch = 4\nmax_grad = 0.25\n\n# 学習データの読み込み\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\ncorpus_test, _, _ = ptb.load_data('test')\nvocab_size = len(word_to_id)\nxs = corpus[:-1]\nts = corpus[1:]\n\n# モデルの生成\nmodel = Rnnlm(vocab_size, wordvec_size, hidden_size)\noptimizer = SGD(lr)\ntrainer = RnnlmTrainer(model, optimizer)\n\n# 勾配クリッピングを適用して学習\ntrainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n            eval_interval=20)\ntrainer.plot(ylim=(0, 500))\n\n# テストデータで評価\nmodel.reset_state()\nppl_test = eval_perplexity(model, corpus_test)\nprint('test perplexity: ', ppl_test)\n\n# パラメータの保存\nmodel.save_params()"
  },
  {
   "cell_type": "markdown",
   "id": "a1b3d452",
   "metadata": {},
   "source": [
    "## clip_grads.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "print('before:', dW1.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('after:', dW1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4f789",
   "metadata": {},
   "source": [
    "## rnn_gradient_graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 2  # ミニバッチサイズ\n",
    "H = 3  # 隠れ状態ベクトルの次元数\n",
    "T = 20  # 時系列データの長さ\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "Wh = np.random.randn(H, H)\n",
    "#Wh = np.random.randn(H, H) * 0.5\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)\n",
    "\n",
    "# グラフの描画\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a123",
   "metadata": {},
   "source": [
    "## train_better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2c456",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common import config\n# GPUで実行する場合は下記のコメントアウトを消去（要cupy）\n# ==============================================\n# config.GPU = True\n# ==============================================\nfrom common.optimizer import SGD\nfrom common.trainer import RnnlmTrainer\nfrom common.util import eval_perplexity, to_gpu\nfrom dataset import ptb\n# Note: BetterRnnlm is defined in the better_rnnlm.py cell below\n\n\n# ハイパーパラメータの設定\nbatch_size = 20\nwordvec_size = 650\nhidden_size = 650\ntime_size = 35\nlr = 20.0\nmax_epoch = 40\nmax_grad = 0.25\ndropout = 0.5\n\n# 学習データの読み込み\ncorpus, word_to_id, id_to_word = ptb.load_data('train')\ncorpus_val, _, _ = ptb.load_data('val')\ncorpus_test, _, _ = ptb.load_data('test')\n\nif config.GPU:\n    corpus = to_gpu(corpus)\n    corpus_val = to_gpu(corpus_val)\n    corpus_test = to_gpu(corpus_test)\n\nvocab_size = len(word_to_id)\nxs = corpus[:-1]\nts = corpus[1:]\n\nmodel = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\noptimizer = SGD(lr)\ntrainer = RnnlmTrainer(model, optimizer)\n\nbest_ppl = float('inf')\nfor epoch in range(max_epoch):\n    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n                time_size=time_size, max_grad=max_grad)\n\n    model.reset_state()\n    ppl = eval_perplexity(model, corpus_val)\n    print('valid perplexity: ', ppl)\n\n    if best_ppl > ppl:\n        best_ppl = ppl\n        model.save_params()\n    else:\n        lr /= 4.0\n        optimizer.lr = lr\n\n    model.reset_state()\n    print('-' * 50)\n\n\n# テストデータでの評価\nmodel.reset_state()\nppl_test = eval_perplexity(model, corpus_test)\nprint('test perplexity: ', ppl_test)"
  },
  {
   "cell_type": "markdown",
   "id": "a4b7d123",
   "metadata": {},
   "source": [
    "## rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5e234",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.time_layers import *\nfrom common.base_model import BaseModel\n\n\nclass Rnnlm(BaseModel):\n    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        # 重みの初期化\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b = np.zeros(4 * H).astype('f')\n        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        # レイヤの生成\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n            TimeAffine(affine_W, affine_b)\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.lstm_layer = self.layers[1]\n\n        # すべての重みと勾配をリストにまとめる\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def predict(self, xs):\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        return xs\n\n    def forward(self, xs, ts):\n        score = self.predict(xs)\n        loss = self.loss_layer.forward(score, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        self.lstm_layer.reset_state()"
  },
  {
   "cell_type": "markdown",
   "id": "c1a2b345",
   "metadata": {},
   "source": [
    "## better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7e890",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.time_layers import *\nfrom common.np import *  # import numpy as np\nfrom common.base_model import BaseModel\n\n\nclass BetterRnnlm(BaseModel):\n    '''\n     LSTMレイヤを2層利用し、各層にDropoutを使うモデル\n     [1]で提案されたモデルをベースとし、weight tying[2][3]を利用\n\n     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n    '''\n    def __init__(self, vocab_size=10000, wordvec_size=650,\n                 hidden_size=650, dropout_ratio=0.5):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b1 = np.zeros(4 * H).astype('f')\n        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b2 = np.zeros(4 * H).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeDropout(dropout_ratio),\n            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n            TimeDropout(dropout_ratio),\n            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n            TimeDropout(dropout_ratio),\n            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.lstm_layers = [self.layers[2], self.layers[4]]\n        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def predict(self, xs, train_flg=False):\n        for layer in self.drop_layers:\n            layer.train_flg = train_flg\n\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        return xs\n\n    def forward(self, xs, ts, train_flg=True):\n        score = self.predict(xs, train_flg)\n        loss = self.loss_layer.forward(score, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        for layer in self.lstm_layers:\n            layer.reset_state()"
  },
  {
   "cell_type": "markdown",
   "id": "e8b9c456",
   "metadata": {},
   "source": [
    "## eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b234",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom dataset import ptb\nfrom common.util import eval_perplexity\n# Note: Rnnlm and BetterRnnlm classes are defined in the cells above\n\n\nif __name__ == '__main__':\n    model = Rnnlm()\n    #model = BetterRnnlm()\n\n    # 学習済みのパラメータの読み込み\n    model.load_params()\n\n    corpus, _, _ = ptb.load_data('test')\n\n    model.reset_state()\n    ppl_test = eval_perplexity(model, corpus)\n    print('test perplexity: ', ppl_test)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}