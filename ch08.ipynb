{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b4e1e",
   "metadata": {},
   "source": [
    "# Chapter 8\n",
    "\n",
    "This notebook contains all Python files from ch08 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8456",
   "metadata": {},
   "source": [
    "## attention_seq2seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8c123",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.time_layers import *\n# Note: Encoder and Seq2seq classes are from ch07, TimeAttention is defined in the attention_layer.py cell below\n\n\nclass AttentionEncoder(Encoder):\n    def forward(self, xs):\n        xs = self.embed.forward(xs)\n        hs = self.lstm.forward(xs)\n        return hs\n\n    def backward(self, dhs):\n        dout = self.lstm.backward(dhs)\n        dout = self.embed.backward(dout)\n        return dout\n\n\nclass AttentionDecoder:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        embed_W = (rn(V, D) / 100).astype('f')\n        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n        lstm_b = np.zeros(4 * H).astype('f')\n        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        self.embed = TimeEmbedding(embed_W)\n        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n        self.attention = TimeAttention()\n        self.affine = TimeAffine(affine_W, affine_b)\n        layers = [self.embed, self.lstm, self.attention, self.affine]\n\n        self.params, self.grads = [], []\n        for layer in layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, enc_hs):\n        h = enc_hs[:,-1]\n        self.lstm.set_state(h)\n\n        out = self.embed.forward(xs)\n        dec_hs = self.lstm.forward(out)\n        c = self.attention.forward(enc_hs, dec_hs)\n        out = np.concatenate((c, dec_hs), axis=2)\n        score = self.affine.forward(out)\n\n        return score\n\n    def backward(self, dscore):\n        dout = self.affine.backward(dscore)\n        N, T, H2 = dout.shape\n        H = H2 // 2\n\n        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n        denc_hs, ddec_hs1 = self.attention.backward(dc)\n        ddec_hs = ddec_hs0 + ddec_hs1\n        dout = self.lstm.backward(ddec_hs)\n        dh = self.lstm.dh\n        denc_hs[:, -1] += dh\n        self.embed.backward(dout)\n\n        return denc_hs\n\n    def generate(self, enc_hs, start_id, sample_size):\n        sampled = []\n        sample_id = start_id\n        h = enc_hs[:, -1]\n        self.lstm.set_state(h)\n\n        for _ in range(sample_size):\n            x = np.array([sample_id]).reshape((1, 1))\n\n            out = self.embed.forward(x)\n            dec_hs = self.lstm.forward(out)\n            c = self.attention.forward(enc_hs, dec_hs)\n            out = np.concatenate((c, dec_hs), axis=2)\n            score = self.affine.forward(out)\n\n            sample_id = np.argmax(score.flatten())\n            sampled.append(sample_id)\n\n        return sampled\n\n\nclass AttentionSeq2seq(Seq2seq):\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        args = vocab_size, wordvec_size, hidden_size\n        self.encoder = AttentionEncoder(*args)\n        self.decoder = AttentionDecoder(*args)\n        self.softmax = TimeSoftmaxWithLoss()\n\n        self.params = self.encoder.params + self.decoder.params\n        self.grads = self.encoder.grads + self.decoder.grads"
  },
  {
   "cell_type": "markdown",
   "id": "a1b3d452",
   "metadata": {},
   "source": [
    "## visualize_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4e567",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nfrom dataset import sequence\nimport matplotlib.pyplot as plt\n# Note: AttentionSeq2seq is defined in the attention_seq2seq.py cell above\n\n\n(x_train, t_train), (x_test, t_test) = \\\n    sequence.load_data('date.txt')\nchar_to_id, id_to_char = sequence.get_vocab()\n\n# Reverse input\nx_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n\nvocab_size = len(char_to_id)\nwordvec_size = 16\nhidden_size = 256\n\nmodel = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\nmodel.load_params()\n\n_idx = 0\ndef visualize(attention_map, row_labels, column_labels):\n    fig, ax = plt.subplots()\n    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n\n    ax.patch.set_facecolor('black')\n    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n    ax.invert_yaxis()\n    ax.set_xticklabels(row_labels, minor=False)\n    ax.set_yticklabels(column_labels, minor=False)\n\n    global _idx\n    _idx += 1\n    plt.show()\n\n\nnp.random.seed(1984)\nfor _ in range(5):\n    idx = [np.random.randint(0, len(x_test))]\n    x = x_test[idx]\n    t = t_test[idx]\n\n    model.forward(x, t)\n    d = model.decoder.attention.attention_weights\n    d = np.array(d)\n    attention_map = d.reshape(d.shape[0], d.shape[2])\n\n    # reverse for print\n    attention_map = attention_map[:,::-1]\n    x = x[:,::-1]\n\n    row_labels = [id_to_char[i] for i in x[0]]\n    column_labels = [id_to_char[i] for i in t[0]]\n    column_labels = column_labels[1:]\n\n    visualize(attention_map, row_labels, column_labels)"
  },
  {
   "cell_type": "markdown",
   "id": "c3d4f789",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f891",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset import sequence\nfrom common.optimizer import Adam\nfrom common.trainer import Trainer\nfrom common.util import eval_seq2seq\n# Note: AttentionSeq2seq is defined in the attention_seq2seq.py cell above\n# Note: Seq2seq and PeekySeq2seq classes are from ch07\n\n\n# データの読み込み\n(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\nchar_to_id, id_to_char = sequence.get_vocab()\n\n# 入力文を反転\nx_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n\n# ハイパーパラメータの設定\nvocab_size = len(char_to_id)\nwordvec_size = 16\nhidden_size = 256\nbatch_size = 128\nmax_epoch = 10\nmax_grad = 5.0\n\nmodel = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n\noptimizer = Adam()\ntrainer = Trainer(model, optimizer)\n\nacc_list = []\nfor epoch in range(max_epoch):\n    trainer.fit(x_train, t_train, max_epoch=1,\n                batch_size=batch_size, max_grad=max_grad)\n\n    correct_num = 0\n    for i in range(len(x_test)):\n        question, correct = x_test[[i]], t_test[[i]]\n        verbose = i < 10\n        correct_num += eval_seq2seq(model, question, correct,\n                                    id_to_char, verbose, is_reverse=True)\n\n    acc = float(correct_num) / len(x_test)\n    acc_list.append(acc)\n    print('val acc %.3f%%' % (acc * 100))\n\n\nmodel.save_params()\n\n# グラフの描画\nx = np.arange(len(acc_list))\nplt.plot(x, acc_list, marker='o')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.ylim(-0.05, 1.05)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a123",
   "metadata": {},
   "source": [
    "## attention_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2c456",
   "metadata": {},
   "outputs": [],
   "source": "# coding: utf-8\nfrom common.np import *  # import numpy as np\nfrom common.layers import Softmax\n\n\nclass WeightSum:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.cache = None\n\n    def forward(self, hs, a):\n        N, T, H = hs.shape\n\n        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n        t = hs * ar\n        c = np.sum(t, axis=1)\n\n        self.cache = (hs, ar)\n        return c\n\n    def backward(self, dc):\n        hs, ar = self.cache\n        N, T, H = hs.shape\n        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n        dar = dt * hs\n        dhs = dt * ar\n        da = np.sum(dar, axis=2)\n\n        return dhs, da\n\n\nclass AttentionWeight:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.softmax = Softmax()\n        self.cache = None\n\n    def forward(self, hs, h):\n        N, T, H = hs.shape\n\n        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n        t = hs * hr\n        s = np.sum(t, axis=2)\n        a = self.softmax.forward(s)\n\n        self.cache = (hs, hr)\n        return a\n\n    def backward(self, da):\n        hs, hr = self.cache\n        N, T, H = hs.shape\n\n        ds = self.softmax.backward(da)\n        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n        dhs = dt * hr\n        dhr = dt * hs\n        dh = np.sum(dhr, axis=1)\n\n        return dhs, dh\n\n\nclass Attention:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.attention_weight_layer = AttentionWeight()\n        self.weight_sum_layer = WeightSum()\n        self.attention_weight = None\n\n    def forward(self, hs, h):\n        a = self.attention_weight_layer.forward(hs, h)\n        out = self.weight_sum_layer.forward(hs, a)\n        self.attention_weight = a\n        return out\n\n    def backward(self, dout):\n        dhs0, da = self.weight_sum_layer.backward(dout)\n        dhs1, dh = self.attention_weight_layer.backward(da)\n        dhs = dhs0 + dhs1\n        return dhs, dh\n\n\nclass TimeAttention:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.layers = None\n        self.attention_weights = None\n\n    def forward(self, hs_enc, hs_dec):\n        N, T, H = hs_dec.shape\n        out = np.empty_like(hs_dec)\n        self.layers = []\n        self.attention_weights = []\n\n        for t in range(T):\n            layer = Attention()\n            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n            self.layers.append(layer)\n            self.attention_weights.append(layer.attention_weight)\n\n        return out\n\n    def backward(self, dout):\n        N, T, H = dout.shape\n        dhs_enc = 0\n        dhs_dec = np.empty_like(dout)\n\n        for t in range(T):\n            layer = self.layers[t]\n            dhs, dh = layer.backward(dout[:, t, :])\n            dhs_enc += dhs\n            dhs_dec[:,t,:] = dh\n\n        return dhs_enc, dhs_dec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}